{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SP21_IST597_week5_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujayk96/IST-597-DL/blob/main/SP21_IST597_week5_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "np.random.seed(248)\n",
        "tf.random.set_seed(248)\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV-3kEaggcO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a720870a-3a3c-4d81-cec2-e417e5d56a69"
      },
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw78jw6pDqSM"
      },
      "source": [
        "#Get number of Gpu's and id's in the system or else you can also use Nvidia-smi in command prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Load MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0], 784))/255.\n",
        "    x_test = np.reshape(x_test, (x_test.shape[0], 784))/255.\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "    x_train1, x_val, y_train1, y_val = train_test_split(x_train, y_train, test_size=0.1)\n",
        "    return (x_train1, y_train1), (x_val, y_val), (x_test, y_test)"
      ],
      "metadata": {
        "id": "QiT5f8b6CL4u"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SJzVI207LGsY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "source": [
        "size_input = 784\n",
        "size_hidden = [128, 64]\n",
        "size_output = 10\n",
        "number_of_train_examples = 60000\n",
        "number_of_test_examples = 10000"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm23CzRihaW0"
      },
      "source": [
        "(X_train, y_train),(X_val, y_val), (X_test, y_test) = load_data()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "xMUgnT2nO8TX",
        "outputId": "1615ec31-bae8-49df-b15f-aff785c381e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54000, 784) (54000, 10)\n",
            "(6000, 784) (6000, 10)\n",
            "(10000, 784) (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(100)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(50)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    # self.W0 = tf.Variable(tf.random.normal([self.size_input, self.size_input]))\n",
        "    # # Initialize biases for hidden layer\n",
        "    # self.b0 = tf.Variable(tf.random.normal([1, self.size_input]))\n",
        "    self.W1 = tf.Variable(tf.math.multiply(tf.random.normal([self.size_input, self.size_hidden[0]]), 0.05) )\n",
        "    # Initialize biases for hidden layer\n",
        "    #print(self.W1)\n",
        "    self.b1 = tf.Variable(tf.math.multiply(tf.random.normal([1, self.size_hidden[0]]), 0.05))\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W2 = tf.Variable(tf.math.multiply(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]), 0.05))\n",
        "    # Initialize biases for output layer\n",
        "    self.b2 = tf.Variable(tf.math.multiply(tf.random.normal([1, self.size_hidden[1]]), 0.05))\n",
        "    self.W3 = tf.Variable(tf.math.multiply(tf.random.normal([self.size_hidden[1], self.size_output]), 0.05))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.math.multiply(tf.random.normal([1, self.size_output]), 0.05))\n",
        "    # Define variables to be updated during backpropagation\n",
        "    #self.variables = [self.W0, self.b0,self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def forward_with_dp(self, X, dp):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output_dropout(X, dp)\n",
        "    else:\n",
        "      self.y = self.compute_output_dropout(X, dp)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    los = tf.keras.losses.CategoricalCrossentropy()(y_true, y_pred)\n",
        "    # print(y_true.shape)\n",
        "    # print(y_pred.shape)\n",
        "    # print(los)\n",
        "    return los\n",
        "\n",
        "  def loss_with_reg(self, y_pred, y_true, lambd):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    m = y_true.shape[0]\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    los = tf.keras.losses.CategoricalCrossentropy()(y_true, y_pred)\n",
        "    # L2_regularization_cost = (tf.reduce_sum(tf.square(self.W0)) + \n",
        "    #                           tf.reduce_sum(tf.square(self.W1)) + \n",
        "    #                           tf.reduce_sum(tf.square(self.W2)) + \n",
        "    #                           tf.reduce_sum(tf.square(self.W3)))*(lambd/(2*m))\n",
        "    \n",
        "    L2_regularization_cost = (tf.reduce_sum(tf.square(self.W1)) + \n",
        "                              tf.reduce_sum(tf.square(self.W2)) + \n",
        "                              tf.reduce_sum(tf.square(self.W3)))*(lambd/(2*m))\n",
        "    \n",
        "    reg_loss = los + L2_regularization_cost\n",
        "    #print(L2_regularization_cost, los, reg_loss)\n",
        "    return reg_loss\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #print(self.W1.shape)\n",
        "    #t1 = self.W1\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      \n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    #t2 = self.W1\n",
        "    # print(\"***************************************************\")\n",
        "    # print(len(grads))\n",
        "    # print(sum(grads[0]), sum(grads[1]), sum(grads[2]), sum(grads[3]), sum(grads[4]), sum(grads[5]))\n",
        "    # print(\"********************************************************************\")\n",
        "    # print(tf.reduce_sum(self.W1))\n",
        "  \n",
        "  def backward_with_dropout(self, X_train, y_train,dp):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #print(self.W1)\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward_with_dp(X_train,dp)\n",
        "      \n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    #print(self.W1)\n",
        "\n",
        "  def backward_with_reg(self, X_train, y_train, reg):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #print(y_train.shape)\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      \n",
        "      current_loss = self.loss_with_reg(predicted, y_train, reg)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables)) \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    Z0 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    A0 = tf.nn.relu(Z0)\n",
        "    A0 = tf.nn.batch_normalization(A0, tf.reduce_mean(A0), tf.math.reduce_variance(A0),None, None, 1e-12)\n",
        "    Z1 = tf.matmul(A0, self.W2) + self.b2\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    A1 = tf.nn.batch_normalization(A1, tf.reduce_mean(A1), tf.math.reduce_variance(A1), None, None, 1e-12)\n",
        "\n",
        "    Z2 = tf.matmul(A1, self.W3) + self.b3\n",
        "\n",
        "    output = tf.nn.softmax(Z2)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def compute_output_dropout(self, X, dp):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = (X_tf - tf.math.reduce_mean(X_tf)) / tf.math.reduce_variance(X_tf)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    ##Z0 = tf.matmul(X_tf, self.W0) + self.b0\n",
        "    #A0 = tf.nn.relu(Z0)\n",
        "    Z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    A1 = tf.nn.dropout(A1, dp)\n",
        "    # D1 = tf.random.uniform([A1.shape[0],A1.shape[1]])\n",
        "    # D1 = D1 < dp\n",
        "    # A1 = tf.where(D1, A1*1.0, A1*0.0)\n",
        "    # Compute output\n",
        "    Z2 = tf.matmul(A1, self.W2) + self.b2\n",
        "    A2 = tf.nn.relu(Z2)\n",
        "    A2 = tf.nn.dropout(A2, dp)\n",
        "    # D2 = tf.random.uniform([A2.shape[0],A2.shape[1]])\n",
        "    # D2 = D2 < dp\n",
        "    # A2 = tf.where(D2, A2*1.0, A2*0.0)\n",
        "    Z3 = tf.matmul(A2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(Z3)\n",
        "    #print(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model reg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZPVUu0YDa-_"
      },
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10\n",
        "seeds = [123,4324,543,5290,9922,3456,1111,9999,8567,9944]\n",
        "seeds1 = [5290]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdMFAuH18Ve0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269ef33a-cdb8-438b-e4e5-17ddd07011a7"
      },
      "source": [
        "history_list = []\n",
        "history_test_acc_0 = []\n",
        "maxpos = lambda x : np.argmax(x)\n",
        "\n",
        "for seed in seeds: \n",
        "  history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'val_loss': []\n",
        "  }\n",
        "  print(\"**********************************************************************************************************************************\")\n",
        "  NUM_EPOCHS = 5\n",
        "  mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "\n",
        "\n",
        "  time_start = time.time()\n",
        "\n",
        "  REG_C = 0\n",
        "  reg = 0.001\n",
        "  dp = 0.7\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(100)\n",
        "\n",
        "    for inputs, outputs in train_ds:\n",
        "      \n",
        "\n",
        "      if REG_C == 0:\n",
        "        # Loss and backward without reg\n",
        "        preds = mlp_on_gpu.forward(inputs)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "        \n",
        "        mlp_on_gpu.backward(inputs, outputs)\n",
        "        \n",
        "      elif REG_C == 1:\n",
        "        # Loss and backward with Regularization\n",
        "        preds = mlp_on_gpu.forward(inputs)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss_with_reg(preds, outputs, reg)\n",
        "        mlp_on_gpu.backward_with_reg(inputs, outputs, dp)\n",
        "      else:\n",
        "        # Loss and backward with Backprop\n",
        "        preds = mlp_on_gpu.forward_with_dp(inputs, dp)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "        mlp_on_gpu.backward_with_dropout(inputs, outputs, reg)\n",
        "\n",
        "    train_loss = np.sum(loss_total_gpu) / X_train.shape[0]  \n",
        "\n",
        "      ###### Train accuracy\n",
        "    preds = mlp_on_gpu.forward(X_train)\n",
        "    yTrueMax = np.array([maxpos(rec) for rec in y_train])\n",
        "    yPredMax = np.array([maxpos(rec) for rec in preds])\n",
        "    train_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "    #print(\"Train accuracy:\", train_acc)\n",
        "\n",
        "    ######## Val loss and accuracy\n",
        "    val_loss = 0\n",
        "    if REG_C == 0:\n",
        "      # Loss and backward without reg\n",
        "      val_preds = mlp_on_gpu.forward(X_val)\n",
        "      val_loss = mlp_on_gpu.loss(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc)        \n",
        "    elif REG_C == 1:\n",
        "      # Loss and backward with Regularization\n",
        "      val_preds = mlp_on_gpu.forward(X_val)\n",
        "      val_loss = mlp_on_gpu.loss_with_reg(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc) \n",
        "    else:\n",
        "      # Loss and backward with Backprop\n",
        "      val_preds = mlp_on_gpu.forward_with_dp(X_val)\n",
        "      val_loss = mlp_on_gpu.loss(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc) \n",
        "\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(loss_total_gpu_val)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    print('Number of Epoch = {} - Average train loss:= {} - Average val loss:= {}, Train Acc:= {}, Val acc:= {}'.format(epoch + 1, train_loss, val_loss, train_acc, val_acc))\n",
        "\n",
        "     \n",
        "       \n",
        "      \n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "  print('\\nTotal time taken (in seconds): {:.2f} , {}'.format(time_taken, seed))\n",
        "\n",
        "\n",
        "\n",
        "  ##### Test Accuracy\n",
        "\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  preds = mlp_on_gpu.forward(X_test)\n",
        "  yTrueMax = np.array([maxpos(rec) for rec in y_test])\n",
        "  yPredMax = np.array([maxpos(rec) for rec in preds])\n",
        "  test_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "  print(\"Test Accuracy:\", test_acc)\n",
        "  history_test_acc_0.append(test_acc)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.0038568468447084782 - Average val loss:= 0.18799804151058197, Train Acc:= 0.950925925925926, Val acc:= 0.9466666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.0015673573811848958 - Average val loss:= 0.13317474722862244, Train Acc:= 0.970462962962963, Val acc:= 0.9606666666666667\n",
            "Number of Epoch = 3 - Average train loss:= 0.0010824854815447771 - Average val loss:= 0.11253788322210312, Train Acc:= 0.9778148148148148, Val acc:= 0.9653333333333334\n",
            "Number of Epoch = 4 - Average train loss:= 0.0008092892964680989 - Average val loss:= 0.10302722454071045, Train Acc:= 0.9835, Val acc:= 0.97\n",
            "Number of Epoch = 5 - Average train loss:= 0.0006308597282127097 - Average val loss:= 0.09662912040948868, Train Acc:= 0.986037037037037, Val acc:= 0.9713333333333334\n",
            "\n",
            "Total time taken (in seconds): 164.06 , 123\n",
            "Test Accuracy: 0.973\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.0037745706063729745 - Average val loss:= 0.19008642435073853, Train Acc:= 0.9506666666666667, Val acc:= 0.9443333333333334\n",
            "Number of Epoch = 2 - Average train loss:= 0.0015490036010742188 - Average val loss:= 0.1356656551361084, Train Acc:= 0.9693888888888889, Val acc:= 0.958\n",
            "Number of Epoch = 3 - Average train loss:= 0.0010710423787434896 - Average val loss:= 0.11523840576410294, Train Acc:= 0.977462962962963, Val acc:= 0.9648333333333333\n",
            "Number of Epoch = 4 - Average train loss:= 0.0007994648968731915 - Average val loss:= 0.10233679413795471, Train Acc:= 0.9835555555555555, Val acc:= 0.9693333333333334\n",
            "Number of Epoch = 5 - Average train loss:= 0.0006082173806649668 - Average val loss:= 0.09298063814640045, Train Acc:= 0.9878888888888889, Val acc:= 0.9728333333333333\n",
            "\n",
            "Total time taken (in seconds): 155.16 , 4324\n",
            "Test Accuracy: 0.9739\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.003743068836353443 - Average val loss:= 0.18858420848846436, Train Acc:= 0.9515185185185185, Val acc:= 0.9435\n",
            "Number of Epoch = 2 - Average train loss:= 0.0015109819482873987 - Average val loss:= 0.1260863095521927, Train Acc:= 0.9713888888888889, Val acc:= 0.9613333333333334\n",
            "Number of Epoch = 3 - Average train loss:= 0.0010364564966272424 - Average val loss:= 0.10840203613042831, Train Acc:= 0.9791851851851852, Val acc:= 0.967\n",
            "Number of Epoch = 4 - Average train loss:= 0.000775504642062717 - Average val loss:= 0.09777199476957321, Train Acc:= 0.9845185185185186, Val acc:= 0.9711666666666666\n",
            "Number of Epoch = 5 - Average train loss:= 0.000606369724980107 - Average val loss:= 0.09092631191015244, Train Acc:= 0.9880740740740741, Val acc:= 0.9738333333333333\n",
            "\n",
            "Total time taken (in seconds): 155.90 , 543\n",
            "Test Accuracy: 0.9741\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.0036995923077618635 - Average val loss:= 0.1792229562997818, Train Acc:= 0.9546111111111111, Val acc:= 0.9508333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.0014574772870099104 - Average val loss:= 0.12855316698551178, Train Acc:= 0.9708888888888889, Val acc:= 0.9628333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.001016092441700123 - Average val loss:= 0.10798390954732895, Train Acc:= 0.9786666666666667, Val acc:= 0.9675\n",
            "Number of Epoch = 4 - Average train loss:= 0.0007668316452591507 - Average val loss:= 0.09943247586488724, Train Acc:= 0.9829074074074075, Val acc:= 0.9705\n",
            "Number of Epoch = 5 - Average train loss:= 0.0005991921601472078 - Average val loss:= 0.09128007292747498, Train Acc:= 0.9872407407407408, Val acc:= 0.9728333333333333\n",
            "\n",
            "Total time taken (in seconds): 154.49 , 5290\n",
            "Test Accuracy: 0.9714\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.0037147115071614585 - Average val loss:= 0.19005230069160461, Train Acc:= 0.9522037037037037, Val acc:= 0.9448333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.001516495457401982 - Average val loss:= 0.13843576610088348, Train Acc:= 0.9696481481481481, Val acc:= 0.9598333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.00106166175559715 - Average val loss:= 0.11749745905399323, Train Acc:= 0.9783148148148149, Val acc:= 0.965\n",
            "Number of Epoch = 4 - Average train loss:= 0.0008089398984555845 - Average val loss:= 0.10647542029619217, Train Acc:= 0.9834074074074074, Val acc:= 0.9681666666666666\n",
            "Number of Epoch = 5 - Average train loss:= 0.0006385733286539714 - Average val loss:= 0.0980781838297844, Train Acc:= 0.9871666666666666, Val acc:= 0.97\n",
            "\n",
            "Total time taken (in seconds): 154.96 , 9922\n",
            "Test Accuracy: 0.9721\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.003770690352828414 - Average val loss:= 0.1893904209136963, Train Acc:= 0.9527777777777777, Val acc:= 0.9446666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.0015054264775028936 - Average val loss:= 0.12985077500343323, Train Acc:= 0.9711296296296297, Val acc:= 0.9628333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.0010355639987521701 - Average val loss:= 0.10821694135665894, Train Acc:= 0.9796666666666667, Val acc:= 0.967\n",
            "Number of Epoch = 4 - Average train loss:= 0.0007769225085223163 - Average val loss:= 0.09770703315734863, Train Acc:= 0.9849259259259259, Val acc:= 0.97\n",
            "Number of Epoch = 5 - Average train loss:= 0.0005986789420798973 - Average val loss:= 0.09083303064107895, Train Acc:= 0.9884444444444445, Val acc:= 0.9716666666666667\n",
            "\n",
            "Total time taken (in seconds): 154.77 , 3456\n",
            "Test Accuracy: 0.9742\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.0038210488778573494 - Average val loss:= 0.18650884926319122, Train Acc:= 0.9513518518518519, Val acc:= 0.946\n",
            "Number of Epoch = 2 - Average train loss:= 0.0015200538635253907 - Average val loss:= 0.12867936491966248, Train Acc:= 0.9707222222222223, Val acc:= 0.9635\n",
            "Number of Epoch = 3 - Average train loss:= 0.0010563557236282913 - Average val loss:= 0.10613858699798584, Train Acc:= 0.979574074074074, Val acc:= 0.969\n",
            "Number of Epoch = 4 - Average train loss:= 0.0008093988630506727 - Average val loss:= 0.0982687696814537, Train Acc:= 0.9840740740740741, Val acc:= 0.9715\n",
            "Number of Epoch = 5 - Average train loss:= 0.0006414977886058666 - Average val loss:= 0.09128141403198242, Train Acc:= 0.9874814814814815, Val acc:= 0.9725\n",
            "\n",
            "Total time taken (in seconds): 155.38 , 1111\n",
            "Test Accuracy: 0.9742\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.0038796265213577835 - Average val loss:= 0.19376271963119507, Train Acc:= 0.9512962962962963, Val acc:= 0.9438333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.001547830793592665 - Average val loss:= 0.13351233303546906, Train Acc:= 0.9694444444444444, Val acc:= 0.9601666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.001068084928724501 - Average val loss:= 0.11191074550151825, Train Acc:= 0.9783333333333334, Val acc:= 0.9651666666666666\n",
            "Number of Epoch = 4 - Average train loss:= 0.0008025699191623264 - Average val loss:= 0.1013568714261055, Train Acc:= 0.9841111111111112, Val acc:= 0.9673333333333334\n",
            "Number of Epoch = 5 - Average train loss:= 0.0006287111352991175 - Average val loss:= 0.09300889074802399, Train Acc:= 0.9878888888888889, Val acc:= 0.9713333333333334\n",
            "\n",
            "Total time taken (in seconds): 168.93 , 9999\n",
            "Test Accuracy: 0.9732\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.003797721580222801 - Average val loss:= 0.1863362193107605, Train Acc:= 0.9518518518518518, Val acc:= 0.9478333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.0015324018972891348 - Average val loss:= 0.1286385953426361, Train Acc:= 0.9707962962962963, Val acc:= 0.9631666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.0010671101322880497 - Average val loss:= 0.10680333524942398, Train Acc:= 0.979037037037037, Val acc:= 0.9696666666666667\n",
            "Number of Epoch = 4 - Average train loss:= 0.0008051655380814164 - Average val loss:= 0.09732314199209213, Train Acc:= 0.9840555555555556, Val acc:= 0.9723333333333334\n",
            "Number of Epoch = 5 - Average train loss:= 0.0006283541078920717 - Average val loss:= 0.09136494994163513, Train Acc:= 0.9877592592592592, Val acc:= 0.9723333333333334\n",
            "\n",
            "Total time taken (in seconds): 156.07 , 8567\n",
            "Test Accuracy: 0.9745\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.0036936976114908854 - Average val loss:= 0.1871107816696167, Train Acc:= 0.9527777777777777, Val acc:= 0.9471666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.001470763877586082 - Average val loss:= 0.12902267277240753, Train Acc:= 0.9706481481481481, Val acc:= 0.9621666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.001008745687979239 - Average val loss:= 0.10785969346761703, Train Acc:= 0.9797962962962963, Val acc:= 0.9675\n",
            "Number of Epoch = 4 - Average train loss:= 0.0007574630313449436 - Average val loss:= 0.09605724364519119, Train Acc:= 0.9850740740740741, Val acc:= 0.9725\n",
            "Number of Epoch = 5 - Average train loss:= 0.0005842681460910374 - Average val loss:= 0.09086539596319199, Train Acc:= 0.9886666666666667, Val acc:= 0.9723333333333334\n",
            "\n",
            "Total time taken (in seconds): 156.00 , 9944\n",
            "Test Accuracy: 0.9711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_list_1 = []\n",
        "history_test_acc_1 = []\n",
        "maxpos = lambda x : np.argmax(x)\n",
        "\n",
        "for seed in seeds: \n",
        "  history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'val_loss': []\n",
        "  }\n",
        "  print(\"**********************************************************************************************************************************\")\n",
        "  NUM_EPOCHS = 5\n",
        "  mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "\n",
        "\n",
        "  time_start = time.time()\n",
        "\n",
        "  REG_C = 1\n",
        "  reg = 0.001\n",
        "  dp = 0.7\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(100)\n",
        "\n",
        "    for inputs, outputs in train_ds:\n",
        "      \n",
        "\n",
        "      if REG_C == 0:\n",
        "        # Loss and backward without reg\n",
        "        preds = mlp_on_gpu.forward(inputs)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "        \n",
        "        mlp_on_gpu.backward(inputs, outputs)\n",
        "        \n",
        "      elif REG_C == 1:\n",
        "        # Loss and backward with Regularization\n",
        "        preds = mlp_on_gpu.forward(inputs)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss_with_reg(preds, outputs, reg)\n",
        "        mlp_on_gpu.backward_with_reg(inputs, outputs, dp)\n",
        "      else:\n",
        "        # Loss and backward with Backprop\n",
        "        preds = mlp_on_gpu.forward_with_dp(inputs, dp)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "        mlp_on_gpu.backward_with_dropout(inputs, outputs, reg)\n",
        "\n",
        "    train_loss = np.sum(loss_total_gpu) / X_train.shape[0]  \n",
        "\n",
        "      ###### Train accuracy\n",
        "    preds = mlp_on_gpu.forward(X_train)\n",
        "    yTrueMax = np.array([maxpos(rec) for rec in y_train])\n",
        "    yPredMax = np.array([maxpos(rec) for rec in preds])\n",
        "    train_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "    #print(\"Train accuracy:\", train_acc)\n",
        "\n",
        "    ######## Val loss and accuracy\n",
        "    val_loss = 0\n",
        "    if REG_C == 0:\n",
        "      # Loss and backward without reg\n",
        "      val_preds = mlp_on_gpu.forward(X_val)\n",
        "      val_loss = mlp_on_gpu.loss(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc)        \n",
        "    elif REG_C == 1:\n",
        "      # Loss and backward with Regularization\n",
        "      val_preds = mlp_on_gpu.forward(X_val)\n",
        "      val_loss = mlp_on_gpu.loss_with_reg(val_preds, y_val, reg)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc) \n",
        "    else:\n",
        "      # Loss and backward with Backprop\n",
        "      val_preds = mlp_on_gpu.forward_with_dp(X_val)\n",
        "      val_loss = mlp_on_gpu.loss(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc) \n",
        "\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(loss_total_gpu_val)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    print('Number of Epoch = {} - Average train loss:= {} - Average val loss:= {}, Train Acc:= {}, Val acc:= {}'.format(epoch + 1, train_loss, val_loss, train_acc, val_acc))\n",
        "\n",
        "     \n",
        "       \n",
        "      \n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "  print('\\nTotal time taken (in seconds): {:.2f} , {}'.format(time_taken, seed))\n",
        "\n",
        "\n",
        "\n",
        "  ##### Test Accuracy\n",
        "\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  preds = mlp_on_gpu.forward(X_test)\n",
        "  yTrueMax = np.array([maxpos(rec) for rec in y_test])\n",
        "  yPredMax = np.array([maxpos(rec) for rec in preds])\n",
        "  test_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "  print(\"Test Accuracy:\", test_acc)\n",
        "  history_test_acc_1.append(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCbCKWoYOWfI",
        "outputId": "23e68699-4779-417c-d1b5-e8ec1526af9f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.018073723687065973 - Average val loss:= 1.4202098846435547, Train Acc:= 0.6619074074074074, Val acc:= 0.669\n",
            "Number of Epoch = 2 - Average train loss:= 0.01218290540907118 - Average val loss:= 1.043540120124817, Train Acc:= 0.7752962962962963, Val acc:= 0.7783333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.00931857751916956 - Average val loss:= 0.8295058608055115, Train Acc:= 0.8252777777777778, Val acc:= 0.8265\n",
            "Number of Epoch = 4 - Average train loss:= 0.007610596268265336 - Average val loss:= 0.6970154047012329, Train Acc:= 0.8508148148148148, Val acc:= 0.8485\n",
            "Number of Epoch = 5 - Average train loss:= 0.006512874461986401 - Average val loss:= 0.6088036894798279, Train Acc:= 0.8668148148148148, Val acc:= 0.8638333333333333\n",
            "\n",
            "Total time taken (in seconds): 169.77 , 123\n",
            "Test Accuracy: 0.87\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.018775421142578123 - Average val loss:= 1.4959056377410889, Train Acc:= 0.6619814814814815, Val acc:= 0.6616666666666666\n",
            "Number of Epoch = 2 - Average train loss:= 0.012768203170211228 - Average val loss:= 1.0920733213424683, Train Acc:= 0.7561851851851852, Val acc:= 0.7633333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.009712764033564815 - Average val loss:= 0.8626500964164734, Train Acc:= 0.8112037037037036, Val acc:= 0.8183333333333334\n",
            "Number of Epoch = 4 - Average train loss:= 0.007878993846752025 - Average val loss:= 0.7176087498664856, Train Acc:= 0.8423888888888889, Val acc:= 0.8465\n",
            "Number of Epoch = 5 - Average train loss:= 0.006685915911639178 - Average val loss:= 0.620608925819397, Train Acc:= 0.8603888888888889, Val acc:= 0.8625\n",
            "\n",
            "Total time taken (in seconds): 161.59 , 4324\n",
            "Test Accuracy: 0.8719\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.017941740813078703 - Average val loss:= 1.4078797101974487, Train Acc:= 0.660962962962963, Val acc:= 0.6546666666666666\n",
            "Number of Epoch = 2 - Average train loss:= 0.012056813557942709 - Average val loss:= 1.0501104593276978, Train Acc:= 0.764425925925926, Val acc:= 0.7548333333333334\n",
            "Number of Epoch = 3 - Average train loss:= 0.009382322523328993 - Average val loss:= 0.8523197770118713, Train Acc:= 0.8108703703703704, Val acc:= 0.806\n",
            "Number of Epoch = 4 - Average train loss:= 0.007782341285988137 - Average val loss:= 0.7249747514724731, Train Acc:= 0.8396666666666667, Val acc:= 0.8386666666666667\n",
            "Number of Epoch = 5 - Average train loss:= 0.006704147904007523 - Average val loss:= 0.6356385946273804, Train Acc:= 0.8581666666666666, Val acc:= 0.8565\n",
            "\n",
            "Total time taken (in seconds): 170.60 , 543\n",
            "Test Accuracy: 0.8666\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.018733133951822918 - Average val loss:= 1.4859585762023926, Train Acc:= 0.6444814814814814, Val acc:= 0.647\n",
            "Number of Epoch = 2 - Average train loss:= 0.012639893708405672 - Average val loss:= 1.0789049863815308, Train Acc:= 0.7767777777777778, Val acc:= 0.7766666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.009592688666449653 - Average val loss:= 0.8506916761398315, Train Acc:= 0.8254814814814815, Val acc:= 0.8266666666666667\n",
            "Number of Epoch = 4 - Average train loss:= 0.007794374254014757 - Average val loss:= 0.708935558795929, Train Acc:= 0.8496851851851852, Val acc:= 0.8516666666666667\n",
            "Number of Epoch = 5 - Average train loss:= 0.006630580760814526 - Average val loss:= 0.6134277582168579, Train Acc:= 0.8657037037037038, Val acc:= 0.868\n",
            "\n",
            "Total time taken (in seconds): 160.86 , 5290\n",
            "Test Accuracy: 0.8664\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.01810201574254919 - Average val loss:= 1.4006623029708862, Train Acc:= 0.6809814814814815, Val acc:= 0.6821666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.011825176097728587 - Average val loss:= 0.9990600347518921, Train Acc:= 0.7854629629629629, Val acc:= 0.788\n",
            "Number of Epoch = 3 - Average train loss:= 0.008922027587890626 - Average val loss:= 0.7944027781486511, Train Acc:= 0.8267222222222222, Val acc:= 0.8311666666666667\n",
            "Number of Epoch = 4 - Average train loss:= 0.007317272044994213 - Average val loss:= 0.6713406443595886, Train Acc:= 0.8508703703703704, Val acc:= 0.852\n",
            "Number of Epoch = 5 - Average train loss:= 0.006298171431929977 - Average val loss:= 0.5889830589294434, Train Acc:= 0.8659629629629629, Val acc:= 0.8655\n",
            "\n",
            "Total time taken (in seconds): 160.44 , 9922\n",
            "Test Accuracy: 0.8702\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.017873934145326967 - Average val loss:= 1.3854676485061646, Train Acc:= 0.7067777777777777, Val acc:= 0.7098333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.011819611725983796 - Average val loss:= 1.0051270723342896, Train Acc:= 0.7947407407407407, Val acc:= 0.7991666666666667\n",
            "Number of Epoch = 3 - Average train loss:= 0.009002147533275462 - Average val loss:= 0.8002183437347412, Train Acc:= 0.8331666666666667, Val acc:= 0.8385\n",
            "Number of Epoch = 4 - Average train loss:= 0.007371509693287037 - Average val loss:= 0.6738471984863281, Train Acc:= 0.8540740740740741, Val acc:= 0.856\n",
            "Number of Epoch = 5 - Average train loss:= 0.006320126003689236 - Average val loss:= 0.5890874266624451, Train Acc:= 0.8685555555555555, Val acc:= 0.8708333333333333\n",
            "\n",
            "Total time taken (in seconds): 160.27 , 3456\n",
            "Test Accuracy: 0.8758\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.01807767175745081 - Average val loss:= 1.3994574546813965, Train Acc:= 0.6765, Val acc:= 0.6856666666666666\n",
            "Number of Epoch = 2 - Average train loss:= 0.011850186948423033 - Average val loss:= 1.009080171585083, Train Acc:= 0.7900185185185186, Val acc:= 0.792\n",
            "Number of Epoch = 3 - Average train loss:= 0.008994300559714988 - Average val loss:= 0.800605297088623, Train Acc:= 0.8292962962962963, Val acc:= 0.8346666666666667\n",
            "Number of Epoch = 4 - Average train loss:= 0.0073458257604528355 - Average val loss:= 0.6717761158943176, Train Acc:= 0.8511296296296297, Val acc:= 0.8555\n",
            "Number of Epoch = 5 - Average train loss:= 0.006279634263780382 - Average val loss:= 0.5855565071105957, Train Acc:= 0.8662962962962963, Val acc:= 0.8691666666666666\n",
            "\n",
            "Total time taken (in seconds): 160.57 , 1111\n",
            "Test Accuracy: 0.8732\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.017988017894603588 - Average val loss:= 1.3937126398086548, Train Acc:= 0.6867222222222222, Val acc:= 0.6856666666666666\n",
            "Number of Epoch = 2 - Average train loss:= 0.011843765823929398 - Average val loss:= 1.0085216760635376, Train Acc:= 0.7888888888888889, Val acc:= 0.7921666666666667\n",
            "Number of Epoch = 3 - Average train loss:= 0.008984804506655092 - Average val loss:= 0.8021366000175476, Train Acc:= 0.8312962962962963, Val acc:= 0.8318333333333333\n",
            "Number of Epoch = 4 - Average train loss:= 0.007356337935836227 - Average val loss:= 0.676734447479248, Train Acc:= 0.8518148148148148, Val acc:= 0.8533333333333334\n",
            "Number of Epoch = 5 - Average train loss:= 0.0063265064380787035 - Average val loss:= 0.5938447713851929, Train Acc:= 0.8654444444444445, Val acc:= 0.8655\n",
            "\n",
            "Total time taken (in seconds): 160.32 , 9999\n",
            "Test Accuracy: 0.8738\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.018076819525824652 - Average val loss:= 1.39815092086792, Train Acc:= 0.6599074074074074, Val acc:= 0.6623333333333333\n",
            "Number of Epoch = 2 - Average train loss:= 0.01190225671838831 - Average val loss:= 1.0151880979537964, Train Acc:= 0.7796481481481482, Val acc:= 0.7831666666666667\n",
            "Number of Epoch = 3 - Average train loss:= 0.009078729700159143 - Average val loss:= 0.807303786277771, Train Acc:= 0.8309814814814814, Val acc:= 0.8333333333333334\n",
            "Number of Epoch = 4 - Average train loss:= 0.0074253223560474535 - Average val loss:= 0.6770479679107666, Train Acc:= 0.8564074074074074, Val acc:= 0.8588333333333333\n",
            "Number of Epoch = 5 - Average train loss:= 0.006347725197120949 - Average val loss:= 0.5887837409973145, Train Acc:= 0.8712777777777778, Val acc:= 0.8771666666666667\n",
            "\n",
            "Total time taken (in seconds): 161.01 , 8567\n",
            "Test Accuracy: 0.8792\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.017562193693938077 - Average val loss:= 1.3761130571365356, Train Acc:= 0.6597222222222222, Val acc:= 0.6588333333333334\n",
            "Number of Epoch = 2 - Average train loss:= 0.011823833324291088 - Average val loss:= 1.0210657119750977, Train Acc:= 0.7650925925925925, Val acc:= 0.7598333333333334\n",
            "Number of Epoch = 3 - Average train loss:= 0.009167247630931713 - Average val loss:= 0.824370801448822, Train Acc:= 0.8168703703703704, Val acc:= 0.8123333333333334\n",
            "Number of Epoch = 4 - Average train loss:= 0.0075749059606481485 - Average val loss:= 0.6985186338424683, Train Acc:= 0.8438518518518519, Val acc:= 0.8386666666666667\n",
            "Number of Epoch = 5 - Average train loss:= 0.006515911526150174 - Average val loss:= 0.612278163433075, Train Acc:= 0.862037037037037, Val acc:= 0.8563333333333333\n",
            "\n",
            "Total time taken (in seconds): 160.70 , 9944\n",
            "Test Accuracy: 0.8665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_list_2 = []\n",
        "history_test_acc_2 = []\n",
        "maxpos = lambda x : np.argmax(x)\n",
        "\n",
        "for seed in seeds: \n",
        "  history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'val_loss': []\n",
        "  }\n",
        "  print(\"**********************************************************************************************************************************\")\n",
        "  NUM_EPOCHS = 5\n",
        "  mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "\n",
        "\n",
        "  time_start = time.time()\n",
        "\n",
        "  REG_C = 2\n",
        "  reg = 0.001\n",
        "  dp = 0.7\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(100)\n",
        "\n",
        "    for inputs, outputs in train_ds:\n",
        "      \n",
        "\n",
        "      if REG_C == 0:\n",
        "        # Loss and backward without reg\n",
        "        preds = mlp_on_gpu.forward(inputs)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "        \n",
        "        mlp_on_gpu.backward(inputs, outputs)\n",
        "        \n",
        "      elif REG_C == 1:\n",
        "        # Loss and backward with Regularization\n",
        "        preds = mlp_on_gpu.forward(inputs)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss_with_reg(preds, outputs, reg)\n",
        "        mlp_on_gpu.backward_with_reg(inputs, outputs, dp)\n",
        "      else:\n",
        "        # Loss and backward with Backprop\n",
        "        preds = mlp_on_gpu.forward_with_dp(inputs, dp)\n",
        "        loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "        mlp_on_gpu.backward_with_dropout(inputs, outputs, reg)\n",
        "\n",
        "    train_loss = np.sum(loss_total_gpu) / X_train.shape[0]  \n",
        "\n",
        "      ###### Train accuracy\n",
        "    preds = mlp_on_gpu.forward(X_train)\n",
        "    yTrueMax = np.array([maxpos(rec) for rec in y_train])\n",
        "    yPredMax = np.array([maxpos(rec) for rec in preds])\n",
        "    train_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "    #print(\"Train accuracy:\", train_acc)\n",
        "\n",
        "    ######## Val loss and accuracy\n",
        "    val_loss = 0\n",
        "    if REG_C == 0:\n",
        "      # Loss and backward without reg\n",
        "      val_preds = mlp_on_gpu.forward(X_val)\n",
        "      val_loss = mlp_on_gpu.loss(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc)        \n",
        "    elif REG_C == 1:\n",
        "      # Loss and backward with Regularization\n",
        "      val_preds = mlp_on_gpu.forward(X_val)\n",
        "      val_loss = mlp_on_gpu.loss_with_reg(val_preds, y_val, reg)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc) \n",
        "    else:\n",
        "      # Loss and backward with Backprop\n",
        "      val_preds = mlp_on_gpu.forward_with_dp(X_val, dp)\n",
        "      val_loss = mlp_on_gpu.loss(val_preds, y_val)\n",
        "      valyTrueMax = np.array([maxpos(rec) for rec in y_val])\n",
        "      valyPredMax = np.array([maxpos(rec) for rec in val_preds])\n",
        "      val_acc = sum(valyPredMax == valyTrueMax)/len(valyPredMax)\n",
        "      #print(\"val accuracy:\", val_acc) \n",
        "\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(loss_total_gpu_val)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    print('Number of Epoch = {} - Average train loss:= {} - Average val loss:= {}, Train Acc:= {}, Val acc:= {}'.format(epoch + 1, train_loss, val_loss, train_acc, val_acc))\n",
        "\n",
        "     \n",
        "       \n",
        "      \n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "  print('\\nTotal time taken (in seconds): {:.2f} , {}'.format(time_taken, seed))\n",
        "\n",
        "\n",
        "\n",
        "  ##### Test Accuracy\n",
        "\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  preds = mlp_on_gpu.forward(X_test)\n",
        "  yTrueMax = np.array([maxpos(rec) for rec in y_test])\n",
        "  yPredMax = np.array([maxpos(rec) for rec in preds])\n",
        "  test_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "  print(\"Test Accuracy:\", test_acc)\n",
        "  history_test_acc_2.append(test_acc)"
      ],
      "metadata": {
        "id": "sqgizto5OWV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4fbd59-3529-4007-c191-062dc36a1d07"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.02311183449074074 - Average val loss:= 2.3054287433624268, Train Acc:= 0.11998148148148148, Val acc:= 0.1085\n",
            "Number of Epoch = 2 - Average train loss:= 0.023000764069733795 - Average val loss:= 2.2904915809631348, Train Acc:= 0.2022037037037037, Val acc:= 0.12133333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.02286280201099537 - Average val loss:= 2.2775769233703613, Train Acc:= 0.30524074074074076, Val acc:= 0.1375\n",
            "Number of Epoch = 4 - Average train loss:= 0.02272843650535301 - Average val loss:= 2.2646780014038086, Train Acc:= 0.4009814814814815, Val acc:= 0.14783333333333334\n",
            "Number of Epoch = 5 - Average train loss:= 0.022540418836805555 - Average val loss:= 2.2402184009552, Train Acc:= 0.4734259259259259, Val acc:= 0.16683333333333333\n",
            "\n",
            "Total time taken (in seconds): 149.75 , 123\n",
            "Test Accuracy: 0.484\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.0231475671838831 - Average val loss:= 2.310955762863159, Train Acc:= 0.13398148148148148, Val acc:= 0.1105\n",
            "Number of Epoch = 2 - Average train loss:= 0.023023548267505786 - Average val loss:= 2.297816753387451, Train Acc:= 0.21201851851851852, Val acc:= 0.11933333333333333\n",
            "Number of Epoch = 3 - Average train loss:= 0.022919829191984955 - Average val loss:= 2.2800450325012207, Train Acc:= 0.286, Val acc:= 0.13183333333333333\n",
            "Number of Epoch = 4 - Average train loss:= 0.02277777777777778 - Average val loss:= 2.264991283416748, Train Acc:= 0.3479259259259259, Val acc:= 0.1475\n",
            "Number of Epoch = 5 - Average train loss:= 0.02260253002025463 - Average val loss:= 2.2469100952148438, Train Acc:= 0.4009259259259259, Val acc:= 0.17516666666666666\n",
            "\n",
            "Total time taken (in seconds): 147.70 , 4324\n",
            "Test Accuracy: 0.399\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.0230749353479456 - Average val loss:= 2.3074896335601807, Train Acc:= 0.1645185185185185, Val acc:= 0.1115\n",
            "Number of Epoch = 2 - Average train loss:= 0.022985636393229168 - Average val loss:= 2.290968179702759, Train Acc:= 0.2347962962962963, Val acc:= 0.135\n",
            "Number of Epoch = 3 - Average train loss:= 0.022876218442563656 - Average val loss:= 2.285614252090454, Train Acc:= 0.306, Val acc:= 0.14033333333333334\n",
            "Number of Epoch = 4 - Average train loss:= 0.022768473307291665 - Average val loss:= 2.2701306343078613, Train Acc:= 0.3724074074074074, Val acc:= 0.15283333333333332\n",
            "Number of Epoch = 5 - Average train loss:= 0.022621866861979166 - Average val loss:= 2.255812168121338, Train Acc:= 0.42744444444444446, Val acc:= 0.1655\n",
            "\n",
            "Total time taken (in seconds): 146.49 , 543\n",
            "Test Accuracy: 0.4336\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.02314592827690972 - Average val loss:= 2.3141748905181885, Train Acc:= 0.16662962962962963, Val acc:= 0.10833333333333334\n",
            "Number of Epoch = 2 - Average train loss:= 0.02301251446759259 - Average val loss:= 2.2972652912139893, Train Acc:= 0.24896296296296297, Val acc:= 0.1245\n",
            "Number of Epoch = 3 - Average train loss:= 0.022884953251591435 - Average val loss:= 2.2780601978302, Train Acc:= 0.3456851851851852, Val acc:= 0.1385\n",
            "Number of Epoch = 4 - Average train loss:= 0.02274884259259259 - Average val loss:= 2.2665255069732666, Train Acc:= 0.42692592592592593, Val acc:= 0.156\n",
            "Number of Epoch = 5 - Average train loss:= 0.02254990867332176 - Average val loss:= 2.247994899749756, Train Acc:= 0.4845740740740741, Val acc:= 0.16783333333333333\n",
            "\n",
            "Total time taken (in seconds): 146.46 , 5290\n",
            "Test Accuracy: 0.4874\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.02308912941261574 - Average val loss:= 2.3056559562683105, Train Acc:= 0.16292592592592592, Val acc:= 0.11316666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.022998417607060184 - Average val loss:= 2.2958877086639404, Train Acc:= 0.2584444444444444, Val acc:= 0.1235\n",
            "Number of Epoch = 3 - Average train loss:= 0.022901844165943288 - Average val loss:= 2.2910146713256836, Train Acc:= 0.3491111111111111, Val acc:= 0.1205\n",
            "Number of Epoch = 4 - Average train loss:= 0.02280371997974537 - Average val loss:= 2.277524471282959, Train Acc:= 0.4232592592592593, Val acc:= 0.1435\n",
            "Number of Epoch = 5 - Average train loss:= 0.02267548285590278 - Average val loss:= 2.260820150375366, Train Acc:= 0.47685185185185186, Val acc:= 0.16283333333333333\n",
            "\n",
            "Total time taken (in seconds): 155.94 , 9922\n",
            "Test Accuracy: 0.4825\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.023038456669560184 - Average val loss:= 2.2989776134490967, Train Acc:= 0.21322222222222223, Val acc:= 0.11183333333333334\n",
            "Number of Epoch = 2 - Average train loss:= 0.022926601833767362 - Average val loss:= 2.2911901473999023, Train Acc:= 0.30787037037037035, Val acc:= 0.122\n",
            "Number of Epoch = 3 - Average train loss:= 0.022785888671875 - Average val loss:= 2.271895408630371, Train Acc:= 0.3839074074074074, Val acc:= 0.13816666666666666\n",
            "Number of Epoch = 4 - Average train loss:= 0.022637702094184028 - Average val loss:= 2.2589101791381836, Train Acc:= 0.4465925925925926, Val acc:= 0.1535\n",
            "Number of Epoch = 5 - Average train loss:= 0.02245758056640625 - Average val loss:= 2.230393886566162, Train Acc:= 0.4965925925925926, Val acc:= 0.1745\n",
            "\n",
            "Total time taken (in seconds): 144.54 , 3456\n",
            "Test Accuracy: 0.5035\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.02320537425853588 - Average val loss:= 2.3126540184020996, Train Acc:= 0.08383333333333333, Val acc:= 0.1025\n",
            "Number of Epoch = 2 - Average train loss:= 0.023090727629484952 - Average val loss:= 2.303875684738159, Train Acc:= 0.1433888888888889, Val acc:= 0.10466666666666667\n",
            "Number of Epoch = 3 - Average train loss:= 0.02300665283203125 - Average val loss:= 2.2952678203582764, Train Acc:= 0.21738888888888888, Val acc:= 0.11766666666666667\n",
            "Number of Epoch = 4 - Average train loss:= 0.02290356219256366 - Average val loss:= 2.282435178756714, Train Acc:= 0.29085185185185186, Val acc:= 0.13183333333333333\n",
            "Number of Epoch = 5 - Average train loss:= 0.0227750085901331 - Average val loss:= 2.272509813308716, Train Acc:= 0.36085185185185187, Val acc:= 0.15083333333333335\n",
            "\n",
            "Total time taken (in seconds): 143.27 , 1111\n",
            "Test Accuracy: 0.3548\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.02309547706886574 - Average val loss:= 2.3067939281463623, Train Acc:= 0.20262962962962963, Val acc:= 0.11516666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.022981447573061344 - Average val loss:= 2.29095196723938, Train Acc:= 0.28624074074074074, Val acc:= 0.13466666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.022894178602430554 - Average val loss:= 2.2823495864868164, Train Acc:= 0.3672592592592593, Val acc:= 0.14533333333333334\n",
            "Number of Epoch = 4 - Average train loss:= 0.022766020598234953 - Average val loss:= 2.2677102088928223, Train Acc:= 0.4468888888888889, Val acc:= 0.165\n",
            "Number of Epoch = 5 - Average train loss:= 0.02260007279007523 - Average val loss:= 2.2501907348632812, Train Acc:= 0.5059444444444444, Val acc:= 0.18283333333333332\n",
            "\n",
            "Total time taken (in seconds): 148.53 , 9999\n",
            "Test Accuracy: 0.5087\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.0231149744104456 - Average val loss:= 2.3038039207458496, Train Acc:= 0.17851851851851852, Val acc:= 0.11366666666666667\n",
            "Number of Epoch = 2 - Average train loss:= 0.022975321451822916 - Average val loss:= 2.291797637939453, Train Acc:= 0.27825925925925926, Val acc:= 0.129\n",
            "Number of Epoch = 3 - Average train loss:= 0.02283359103732639 - Average val loss:= 2.276237964630127, Train Acc:= 0.3484814814814815, Val acc:= 0.13833333333333334\n",
            "Number of Epoch = 4 - Average train loss:= 0.02265215160228588 - Average val loss:= 2.259641647338867, Train Acc:= 0.3903333333333333, Val acc:= 0.159\n",
            "Number of Epoch = 5 - Average train loss:= 0.022456922743055555 - Average val loss:= 2.234675168991089, Train Acc:= 0.4209074074074074, Val acc:= 0.16766666666666666\n",
            "\n",
            "Total time taken (in seconds): 143.02 , 8567\n",
            "Test Accuracy: 0.4353\n",
            "**********************************************************************************************************************************\n",
            "Number of Epoch = 1 - Average train loss:= 0.023091109664351853 - Average val loss:= 2.302724599838257, Train Acc:= 0.16805555555555557, Val acc:= 0.1055\n",
            "Number of Epoch = 2 - Average train loss:= 0.022953086570457176 - Average val loss:= 2.2897822856903076, Train Acc:= 0.2778703703703704, Val acc:= 0.12416666666666666\n",
            "Number of Epoch = 3 - Average train loss:= 0.02280187762225116 - Average val loss:= 2.2736778259277344, Train Acc:= 0.37596296296296294, Val acc:= 0.1405\n",
            "Number of Epoch = 4 - Average train loss:= 0.022629125524450233 - Average val loss:= 2.251397132873535, Train Acc:= 0.45935185185185184, Val acc:= 0.16066666666666668\n",
            "Number of Epoch = 5 - Average train loss:= 0.022419099030671297 - Average val loss:= 2.2267372608184814, Train Acc:= 0.5201481481481481, Val acc:= 0.18466666666666667\n",
            "\n",
            "Total time taken (in seconds): 143.30 , 9944\n",
            "Test Accuracy: 0.5361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history_test_acc_0)\n",
        "print(history_test_acc_1)\n",
        "print(history_test_acc_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI4eH3PWrJRa",
        "outputId": "8935efee-e5cc-453b-b40b-35259c584aac"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.973, 0.9739, 0.9741, 0.9714, 0.9721, 0.9742, 0.9742, 0.9732, 0.9745, 0.9711]\n",
            "[0.87, 0.8719, 0.8666, 0.8664, 0.8702, 0.8758, 0.8732, 0.8738, 0.8792, 0.8665]\n",
            "[0.484, 0.399, 0.4336, 0.4874, 0.4825, 0.5035, 0.3548, 0.5087, 0.4353, 0.5361]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {\"Normal\":history_test_acc_0,\n",
        "       \"With reg\": history_test_acc_1,\n",
        "       \"With dp\": history_test_acc_2}"
      ],
      "metadata": {
        "id": "iylWPO8brQjF"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot(dic.values())\n",
        "ax.set_xticklabels(dic.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "n2Z5NXB5sFIN",
        "outputId": "e6b12f08-e414-425f-9bd5-d475549a1425"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0, 'Normal'), Text(0, 0, 'With reg'), Text(0, 0, 'With dp')]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in dic.items():\n",
        "  print(k, \":   Mean: \",np.mean(v), \"Variance: \", np.var(v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCQRBHCgsRpz",
        "outputId": "c49d3e0f-c50d-4e93-b872-bc3165e98605"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal :   Mean:  0.97317 Variance:  1.388099999999991e-06\n",
            "With reg :   Mean:  0.8713599999999999 Variance:  1.6488399999999972e-05\n",
            "With dp :   Mean:  0.46249 Variance:  0.0027921249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zwfr3Q1Bsurz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}